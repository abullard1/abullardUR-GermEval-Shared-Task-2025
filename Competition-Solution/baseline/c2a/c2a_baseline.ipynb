{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Baseline system of \"call to action\" detection model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae79a94",
   "metadata": {},
   "source": [
    "The notebook contains the code for the baseline system for the automatic detection of **Calls-to-action** (**subtask 1** of the Shared Task on Harmful Content Detection). A gradient boosting algorithm was chosen for classification, using sentence embeddings and a polarity score as features. The notebook covers the training of the system as well as the prediction on the test data and the evaluation. \n",
    "\n",
    "The programme was tested using Python version 3.12.9. Executing the following two lines of code will install all necessary packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823c87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "pandas==2.2.3\n",
    "spacy==3.8.2\n",
    "scikit-learn==1.6.1\n",
    "textblob==0.15.3\n",
    "textblob-de==0.4.3\n",
    "sentence-transformers==4.1.0\n",
    "nltk==3.9.1\n",
    "numpy==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23898c38-9718-4fc9-b0db-6aad0e447ffb",
   "metadata": {},
   "source": [
    "## 1. Importing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab67e9e-c5fa-426d-97b3-0865e4139861",
   "metadata": {},
   "source": [
    "First, the training data was read in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbb8c1c-abca-4128-a442-01521ef27c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>C2A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>874458912592533</td>\n",
       "      <td>Wenn Du Wert drauf legst, dann erwarten wir di...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>855617527810005</td>\n",
       "      <td>Macht was ihr wollt, aber schreibt nicht \"Wir ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1111407378897684</td>\n",
       "      <td>Tja, wohl etwas schwierig, jetzt das Geld per ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>945847662120324</td>\n",
       "      <td>UInd das Glaube ich nicht,,,in Hotels in Arfri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>993541110684312</td>\n",
       "      <td>und kein TTIP</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                        description    C2A\n",
       "0   874458912592533  Wenn Du Wert drauf legst, dann erwarten wir di...  False\n",
       "1   855617527810005  Macht was ihr wollt, aber schreibt nicht \"Wir ...  False\n",
       "2  1111407378897684  Tja, wohl etwas schwierig, jetzt das Geld per ...  False\n",
       "3   945847662120324  UInd das Glaube ich nicht,,,in Hotels in Arfri...  False\n",
       "4   993541110684312                                      und kein TTIP  False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in training data \n",
    "import pandas as pd\n",
    "\n",
    "filename = \"c2a_train.csv\" # Path needs to be adjusted \n",
    "train_c2a = pd.read_csv(filename, sep=';')\n",
    "train_c2a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea0ada-410c-4e32-965f-437618ee3683",
   "metadata": {},
   "source": [
    "The class distribution of the training data was analysed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6139b02-c6f5-4fc6-8f1f-fcad7c905c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Frequency  Percentage\n",
      "C2A                         \n",
      "False       6177       90.31\n",
      "True         663        9.69\n"
     ]
    }
   ],
   "source": [
    "# Absolute number of instances in each class \n",
    "class_counts_c2a = train_c2a[\"C2A\"].value_counts()\n",
    "\n",
    "# Relative number of instances in each class \n",
    "class_percent_c2a = train_c2a['C2A'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Summarise into a dataframe\n",
    "class_table_c2a = pd.DataFrame({\n",
    "    'Frequency': class_counts_c2a,\n",
    "    'Percentage': class_percent_c2a.round(2)\n",
    "})\n",
    "\n",
    "print(class_table_c2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5161a26-a83d-4cae-88b8-998348d37445",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing and undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116403c",
   "metadata": {},
   "source": [
    "Since the data is unbalanced, undersampling was applied to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c01e286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2A\n",
      "True     663\n",
      "False    663\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "minority_class = train_c2a[train_c2a['C2A'] == True]\n",
    "majority_class = train_c2a[train_c2a['C2A'] == False]\n",
    "\n",
    "majority_undersampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "train_c2a = pd.concat([minority_class, majority_undersampled])\n",
    "\n",
    "print(train_c2a['C2A'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c2c41fb8b313f",
   "metadata": {},
   "source": [
    "The training data was then pre-processed. This included basic cleansing steps, specifically the removal of URLs, hashtags and mentions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5184ebdd6d89db16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T10:47:51.210185Z",
     "start_time": "2024-12-20T10:47:51.146407Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    mention_pattern = r'@\\w+'\n",
    "    hashtag_pattern = r'#\\w+'\n",
    "    combined_pattern = f'({url_pattern})|({mention_pattern})|({hashtag_pattern})'\n",
    "\n",
    "    cleaned_text = re.sub(combined_pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1358cc8189a8bbb",
   "metadata": {},
   "source": [
    "The texts were then lemmatised and tokenised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "824ab226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-3.8.0/de_core_news_md-3.8.0-py3-none-any.whl (44.4 MB)\n",
      "     ---------------------------------------- 0.0/44.4 MB ? eta -:--:--\n",
      "     --------- ----------------------------- 10.5/44.4 MB 72.5 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 26.2/44.4 MB 66.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 41.9/44.4 MB 74.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 44.4/44.4 MB 62.8 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "# Download the Spacy Pipeline for the German language \n",
    "! python -m spacy download de_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df633cd8baaeec79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T10:48:13.122737Z",
     "start_time": "2024-12-20T10:47:52.630616Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Loading the Spacy pipeline for the German language \n",
    "nlp = spacy.load('de_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "219b8315-9c1e-4394-94b5-5d8fc60c9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the lemmatisation function \n",
    "def text_lemmatize_tokenize(texts):\n",
    "    lemmatized = []\n",
    "    for doc in nlp.pipe(texts, batch_size = 50):\n",
    "        tokens = [token.lemma_.lower() for token in doc if not token.is_punct]\n",
    "        lemmatized.append(' '.join(tokens))\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16b65f-1291-411c-b8da-e933de4a39fb",
   "metadata": {},
   "source": [
    "The two functions for removing certain tokens and for lemmatisation were applied to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b383da4-eaa0-4bf2-9dfe-daa2a3a0a281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  id                                        description   C2A\n",
      "13  1043633089008447  die aus sicheren herkunftsländern nach hause s...  True\n",
      "26  1105436602828095  es ist schade um die armen schweineköpfe, dafü...  True\n",
      "31   982671735104583  Frag doch mal das Serbische Volk wieviel Flüch...  True\n",
      "35  1105439379494484  vergrabt ein totes Schwein, hat in Spanien auc...  True\n",
      "39   946634062041684  Wenn die auf dem Mittelmeer von der Marine auf...  True\n"
     ]
    }
   ],
   "source": [
    "# Removing URLs, hashtags and mentions \n",
    "train_c2a['description'] = train_c2a['description'].apply(clean_text)\n",
    "print(train_c2a.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5301423-7184-40c7-abcc-f40a6c414ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  id                                        description   C2A\n",
      "13  1043633089008447  der aus sicher herkunftsländer nach hause schi...  True\n",
      "26  1105436602828095  es sein schade um der arm schweineköpfe dafür ...  True\n",
      "31   982671735104583  frag doch mal der serbisch volk wieviel flücht...  True\n",
      "35  1105439379494484  vergraben ein tot schwein haben in spanien auc...  True\n",
      "39   946634062041684  wenn der auf der mittelmeer von der marine auf...  True\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation and tokenisation \n",
    "train_c2a['description'] = text_lemmatize_tokenize(train_c2a['description'].tolist())\n",
    "print(train_c2a.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf5e3c",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fab82d4a10b5ac",
   "metadata": {},
   "source": [
    "Next, the tweets from the training data were converted into a feature representation. Specifically, polarity was extracted as a feature using the Text Blob library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73d101a037787ce6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T10:50:28.117992Z",
     "start_time": "2024-12-20T10:49:04.580928Z"
    }
   },
   "outputs": [],
   "source": [
    "from textblob_de import TextBlobDE\n",
    "\n",
    "# Function for determining the polarity of a tweet\n",
    "def add_polarity(df):\n",
    "    def calculate_sentiment_features(text):\n",
    "        blob = TextBlobDE(text)\n",
    "        return blob.sentiment.polarity\n",
    "\n",
    "    df[['polarity']] = df['description'].apply(\n",
    "        lambda x: pd.Series(calculate_sentiment_features(x))\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25de6f5dfd1c9bb0",
   "metadata": {},
   "source": [
    "In addition, the tweets were represented as sentence embeddings using a Sentence BERT model. The feature vectors of each tweet consisted of the polarity value and the sentence embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7496b564bcbebfe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T10:51:14.870577Z",
     "start_time": "2024-12-20T10:50:39.948679Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# Load BERT model \n",
    "sentence_model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "# Function for representing tweets as embeddings \n",
    "def add_semantic_features(df, model):\n",
    "    texts = df['description'].astype(str).values.tolist()\n",
    "\n",
    "    embeddings = model.encode(texts, show_progressbar=True)\n",
    "    embeddings_df = pd.DataFrame(embeddings, columns=[f'embedding_{i}' for i in range(embeddings.shape[1])])\n",
    "\n",
    "    df = pd.concat([df.reset_index(drop=True), embeddings_df.reset_index(drop=True)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ecc94b",
   "metadata": {},
   "source": [
    "The features were extracted from the tweets in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c315131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to \\\\na2.hs-\n",
      "[nltk_data]     mittweida.de\\felser\\Wappscfg\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to \\\\na2.hs-\n",
      "[nltk_data]     mittweida.de\\felser\\Wappscfg\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to \\\\na2.hs-\n",
      "[nltk_data]     mittweida.de\\felser\\Wappscfg\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ee2c476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  id                                        description   C2A  \\\n",
      "13  1043633089008447  der aus sicher herkunftsländer nach hause schi...  True   \n",
      "26  1105436602828095  es sein schade um der arm schweineköpfe dafür ...  True   \n",
      "31   982671735104583  frag doch mal der serbisch volk wieviel flücht...  True   \n",
      "35  1105439379494484  vergraben ein tot schwein haben in spanien auc...  True   \n",
      "39   946634062041684  wenn der auf der mittelmeer von der marine auf...  True   \n",
      "\n",
      "    polarity  \n",
      "13       0.7  \n",
      "26      -0.9  \n",
      "31       0.0  \n",
      "35      -1.0  \n",
      "39       0.0  \n"
     ]
    }
   ],
   "source": [
    "# Extraction of the polarity score\n",
    "train_c2a = add_polarity(train_c2a)\n",
    "print(train_c2a.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bf9d819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                        description   C2A  \\\n",
      "0  1043633089008447  der aus sicher herkunftsländer nach hause schi...  True   \n",
      "1  1105436602828095  es sein schade um der arm schweineköpfe dafür ...  True   \n",
      "2   982671735104583  frag doch mal der serbisch volk wieviel flücht...  True   \n",
      "3  1105439379494484  vergraben ein tot schwein haben in spanien auc...  True   \n",
      "4   946634062041684  wenn der auf der mittelmeer von der marine auf...  True   \n",
      "\n",
      "   polarity  embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
      "0       0.7    -0.022992    -0.002712     0.018320    -0.000925     0.031636   \n",
      "1      -0.9    -0.003322     0.006351     0.033073     0.018905     0.022041   \n",
      "2       0.0    -0.042495     0.020839     0.050303     0.010994    -0.010169   \n",
      "3      -1.0    -0.011467    -0.021485    -0.011638    -0.002952     0.046747   \n",
      "4       0.0     0.031033    -0.015447    -0.019592    -0.017471     0.069267   \n",
      "\n",
      "   embedding_5  ...  embedding_502  embedding_503  embedding_504  \\\n",
      "0    -0.007870  ...      -0.024542       0.002422      -0.007820   \n",
      "1    -0.034962  ...      -0.045453      -0.014113       0.030413   \n",
      "2    -0.011223  ...      -0.024964       0.023871      -0.040662   \n",
      "3    -0.064650  ...      -0.047009       0.017941       0.008438   \n",
      "4    -0.028370  ...      -0.038928       0.046043       0.015078   \n",
      "\n",
      "   embedding_505  embedding_506  embedding_507  embedding_508  embedding_509  \\\n",
      "0      -0.011519       0.002768      -0.004132      -0.010917       0.016448   \n",
      "1      -0.011717       0.021045       0.043700      -0.039317      -0.023507   \n",
      "2       0.015950      -0.046105      -0.005228      -0.021532       0.028166   \n",
      "3      -0.000446       0.030870      -0.024324      -0.042853       0.045750   \n",
      "4       0.026399      -0.039006       0.045574       0.009081      -0.004274   \n",
      "\n",
      "   embedding_510  embedding_511  \n",
      "0      -0.000567      -0.005190  \n",
      "1      -0.002253       0.000719  \n",
      "2      -0.011544      -0.000368  \n",
      "3       0.028313       0.014452  \n",
      "4       0.042091      -0.005366  \n",
      "\n",
      "[5 rows x 516 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extraction of embedding features \n",
    "train_c2a = add_semantic_features(train_c2a, sentence_model)\n",
    "print(train_c2a.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b08aa48",
   "metadata": {},
   "source": [
    "## 4. Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60368992cd0e42ed",
   "metadata": {},
   "source": [
    "A gradient boosting classifier was then trained using the extracted features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cf72c9964305467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T10:51:44.511872Z",
     "start_time": "2024-12-20T10:51:20.850177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Restrict training data set to features \n",
    "X_train = train_c2a.drop(columns=['id', 'description', 'C2A'])\n",
    "# Extract labels from training data \n",
    "y_train = train_c2a['C2A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6532fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77a4d2",
   "metadata": {},
   "source": [
    "## 5. Prediction on the test data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87413ced6ae996ba",
   "metadata": {},
   "source": [
    "Now the trained classification model could be used to make a prediction for the test data. To do this, the test data was preprocessed in the same way as the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e99d9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the test data\n",
    "filename = \"c2a_test.csv\" # Path needs to be adjusted \n",
    "test_c2a = pd.read_csv(filename, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de6105aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing URLs, hashtags and mentions \n",
    "test_c2a['description'] = test_c2a['description'].apply(clean_text)\n",
    "\n",
    "# Lemmatisation and tokenisation \n",
    "test_c2a['description'] = text_lemmatize_tokenize(test_c2a['description'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4418ff",
   "metadata": {},
   "source": [
    "Subsequently, the polarity score was also determined for the tweets in the test data and they were converted into sentence embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19d0a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of the polarity score\n",
    "test_c2a = add_polarity(test_c2a)\n",
    "\n",
    "# Extraction of embedding features \n",
    "test_c2a = add_semantic_features(test_c2a, sentence_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938c17e",
   "metadata": {},
   "source": [
    "The feature representation was passed to the gradient boosting model to make a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "931ef65457490c3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T10:51:59.010693Z",
     "start_time": "2024-12-20T10:51:58.932503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Restrict test data set to features \n",
    "X_test = test_c2a.drop(columns=['id', 'description'])\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923ef62",
   "metadata": {},
   "source": [
    "## 6. Evaluation of results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851db623",
   "metadata": {},
   "source": [
    "The predictions based on the test data were compared with the gold standard and evaluation metrics were calculated. The results achieved serve as a guide and baseline for the competition participants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51b306f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the gold standard \n",
    "filename = \"c2a_gold.csv\" # Path needs to be adjusted \n",
    "gold_c2a = pd.read_csv(filename, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f46d5ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the IDs from the test data and the gold standard are in the same order. \n",
    "gold_c2a[\"id\"].tolist() == test_c2a[\"id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "070f8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true label of the tweets was extracted from the gold standard. \n",
    "y_true = gold_c2a['C2A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6add6",
   "metadata": {},
   "source": [
    "The macro F1 measure is used as the evaluation metric for the ranking in the competition leaderboard. In addition, other evaluation metrics such as precision and recall are calculated for the individual classes, as well as the macro and weighted average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d83d88aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.72      0.83      2693\n",
      "        True       0.23      0.78      0.36       289\n",
      "\n",
      "    accuracy                           0.73      2982\n",
      "   macro avg       0.60      0.75      0.59      2982\n",
      "weighted avg       0.90      0.73      0.78      2982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "test_report = classification_report(y_true, y_test_pred)\n",
    "print(\"Test Classification Report:\")\n",
    "print(test_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c2a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
